Video Source:
https://youtu.be/ZpfhtonDML4

VIDEO DESCRIPTION:
HunyuanVideo Avatar installation tutorial & review. Open-source Veo 3! #ai #aivideo #aitools #ainews 

Official page https://hunyuanvideo-avatar.github.io/
Official Github https://github.com/Tencent-Hunyuan/HunyuanVideo-Avatar
Wan2GP https://github.com/deepbeepmeep/Wan2GP

Install Git: https://git-scm.com/downloads
Conda: https://www.anaconda.com/docs/getting-started/miniconda/install 

Free voice cloning and TTS tools:
RVC    • Make UNLIMITED AI Voice Conversions, Train... [https://www.youtube.com/watch?v=ixB9oalT3cQ]
F5    • This free AI Text-to-Speech is insane! Add... [https://www.youtube.com/watch?v=JBGrhIsCyks]
Zonos    • This new AI text-to-speech BEATS EVERYTHIN... [https://www.youtube.com/watch?v=kHJVovkrJ2o]

TRANSCRIPT:
this AI can generate super realistic videos with any input dialogue it's called Hunyen Video Avatar by Tencent and this is completely free and open- source so you can run it offline for unlimited times on your computer it can even handle various emotions and singing and different animation styles so in this video I'm going to go over how to use it plus I'm going to test it out on a variety of different scenes and audio clips plus of course I'll show you how to install this on your computer so you can use it for free for unlimited times offline the nice thing is this even works if you have low VRAM i'll talk more about this in a second now you might have heard of Google's V3 which was released a few weeks ago it went pretty viral because it could directly make videos with audio so you can get people to talk seamlessly in a video just from one prompt well this new Hunyan video avatar is kind of like VO3 but it's free and open source and I think it's even better because you input the audio so this gives you ultimate control on how you want the audio to sound like first of all here are some of their official demos come on it's time to go i booked the table for 8 and I'm not sure exactly where the restaurant is hey Ally relax this isn't work this is a night out so as you can see it not only works with one character but it can also work with multiple characters here's another example and notice that this isn't just a lip sync tool or a face animator which just moves the person's mouth or face you can input any scene and it would animate the entire scene including the person's body so here's an example of a full body scene plus of a person singing smooth green so as you can see not only does it just move her mouth but it animates her entire body it makes her actually play the guitar plus it animates the campfire in the foreground the only flaw here is the notes that she's playing are not actually aligned with the song but I mean even VO3 can't do that i would be shocked if it was actually in sync here's another example [Music] here's another example of talking again it's not just making her head and mouth move but her entire body and the camera is also moving along plus it can also handle different art styles so here are some examples all right now enough demos if you want to see more demos you can check out this official page but for now let's actually try this out so there are two ways to use it you can try it out via their online platform and once you're there you can sign up for free now this is all in Chinese at the moment but if you're using Google Chrome you can just rightclick and then click on translate to English all right so here first of all this is basically text to speech where you can input a transcript and select a voice now there are different voices you can choose from so for example let me just type in this transcript here and then let me select this voice and then click play here is a sample voice all right and now let's try temperamental and graceful here is a simple voice okay so notice that it does have a Chinese accent to it so I usually don't do text to speech i would upload my own audio clip so let's try this out i'm going to upload this audio clip of Jensen Huang talking this is 14 seconds long let me play this for you first how is it possible that that Nvidia became so big building GPUs and so there's an impression that this is what a GPU looks like now this is a GPU this is one of the most advanced GPUs in the world but this is a gamer GPU all right and then for the image let's upload a picture of Jensen Huang now notice that you can't input a prompt here to guide this generation further this is a limitation of the online platform but you can input a prompt in the offline version which I'll show you in a second anyways let's click generate all right here's what we get let me play this for you how is it possible that that Nvidia became so big building GPUs and so there's an impression that this is what a GPU looks like now this is a GPU this is one of the most advanced GPUs in the world but this is a gamer GP now as you can see that was super realistic it even detected where there was emphasis in the audio and it made Jensen speak that out with more emphasis so for example here when he says "This is a GPU." Now this is a GPU this is one of the most and then when he says "But this is a gamer GPU." But this is a gamer GP again it knows to place emphasis when he says those words super realistic now I chose this scene because it's quite tricky he's holding a GPU plus a laptop with the other hand but it's able to animate this entire scene really consistently with very minimal errors all right next I'm going to upload this photo of a man giving a TED talk which I generated with Google's Imagine and then for the audio I'm going to upload a random talk here's what it sounds like what these creators share in common is independence from traditional gatekeepers and aggregators take Caroline Chambers when publishers spurned her proposal for a cookbook deal she took matters into her own hands all right so let's click generate and see what that gives us okay here's what we get what these creators share in common is independence from traditional gatekeepers and aggregators take Caroline Chambers when publishers spurned her proposal for a cookbook deal she took matters into her own hands very nice now notice that at the middle of the scene there there was kind of a cut which is a flaw but actually in the original audio clip that was also a hard cut so it's kind of interesting how it was able to detect that and again notice that he's moving super realistically it's his entire body moving plus the camera also moves a bit his hands both have five fingers everything is just very realistic all right next I wanted to see if it can handle different languages so here's a Spanish audio and then here is my uploaded photo again it's a pretty complex scene there's a lot of people walking around in the background let's see if it can animate this all right here's what I get all right it's not bad you can see it can clearly handle Spanish however there are clear flaws with the humans in the background for example this dude seems to be walking backwards and then you know it's kind of cool that it added another human walking on the left but somehow he just kind of disappears now they claim that this can also handle different emotions so let's test this out i'm going to use this audio clip please just listen for once why does this always happen and now the trick is you need to first generate an image of the person with that emotion already so let's say we have this really angry girl eating ramen let's click generate and see what that gives us all right our generation is ready let's play this please just listen for once why does this always happen and as you can see this is super realistic she is really pissed off it's as if she's actually shouting the audio plus everything else moves really realistically including the raw men very nice let's also try a laughing example so here's the audio oh my god it was way too funny and for this to work we do need to input a photo of someone already laughing so I made this with Google's Imagine let's click generate all right here's what we get let's play this oh my god it was way too funny okay it's not bad the lip sync isn't perfect but it's like 80% there now instead of anger or laughter here's a sad example can we just get on with it already my body is starting to take a toll now this one looks super realistic notice that this person also takes a breath and sniffs based on the audio clip this is synced really well next I also wanted to see if this works with animals so here's an example i didn't choose the fluffy life the fluffy life chose me so as you can see here even with animals it lip syncs perfectly to the audio this is really good next I also wanted to see if it can handle singing so let me upload this acoustic song let me play you what this sounds like first maybe you're thinking of me too maybe that's just what I do by the way I generated this with Refusion which has easily become one of my favorite music generators out there you can use it for free so check it out anyways back to here i'm going to upload this image of a woman playing guitar all right here's what we get thinking of me [Music] all right so first of all the lip sync is perfect she's actually singing out the lyrics also this whole video just looks very fluid and natural her whole body is moving she's even moving the guitar the camera is also slightly moving the lighting and the reflections are just perfect the only flaw is she's not actually plucking the notes from the song but again I don't expect it to do that so overall this is still a very impressive generation i also wanted to see how well it performs with different art styles so here's a Disney Pixar example yes this is his position in two words a little while since he obtained an excellent offer of employment abroad from a rich relative of his and he had made all his arrangements to accept it again this is super good it even detected a breath and made her pause and breathe near the start of the clip two words plus her eyes do blink and she talks really realistically the lip sync is close to perfect now instead of Disney Pixar 3D style let's also see if it can lip-sync anime so here's an anime example now the mouth could be better but overall this is not bad it can definitely handle anime style and again it makes her body also move slightly plus she does blink her eyes so this is not as robotic and rigid as some of the older tools out there all right so let's talk about this versus VO3 if you want to generate a video of someone talking I would prefer this tool over V3 first of all for VO3 while you can upload an image as the start or end frame of the video you can't actually get that person to talk that's probably due to concerns over deep fakes and copyright another limitation about V3 is you can't actually control the voice of the person the only way to generate audio is with a prompt so you can't really generate a consistent character with a consistent voice whereas for this method what you can do is upload any audio you want so you can generate a voice using any voice cloner and texttospech generator out there i've already covered a ton of them on my channel such as F5TS or Zonos or this really popular one called RVC or retrievalbased voice conversion which can do voicetooice this gives you even more control so you can speak out the audio and then convert your voice into any voice you want by the way if you're interested in learning more about these voice tools check out these videos which I will link to in the description below so after you know generating an audio clip you can easily plug this into Hunen video avatar plus upload a reference image of your character to generate a video from that this truly allows you to create videos with consistent characters and consistent voices which you can't really do with Google's V3 also note that I've specified a ton of other face animators or lip-sync tools on my channel before for example live portrait is a really popular one where you can also upload an audio clip and it would animate a face or hello is another one but those tools are just limited to the person's head or mouth i've also featured Echomimic V3 but that tool is also just limited to a person's upper body this is the first open-source and available AI that can animate any scene even like a full body scene just with an audio clip so that's what makes Hunyan Video Avatar so powerful thanks to VidU for sponsoring this video vidu is one of
the top AI video generators out there with a ton of capabilities their latest Q1 model is even better it has improved clarity detail and semantic accuracy making it a powerful tool for content creators and filmmakers it's really good at creating all types of video for example here's a texttovideo generation as you can see it's super realistic and consistent now instead of text to video you can also do image to video where you can upload a start or end frame and as you can see everything is super consistent plus they have a ton of other features like reference to video where you can upload images of objects or characters you want to insert in the video vu just released a new feature for reference to video now you can contribute your own reference images to a public reference library or browse and use references shared by others to generate your videos for example let's say I want to use this character well I can just select it from this reference library and generate a video of this character in just a few clicks this can save you a ton of time especially if you need to generate specific characters or objects vido sets a new standard for AI video creation try it for free via the link in the description below now those were just some of my examples on this online platform next let's look at how we can install this and run it locally for free for unlimited times on your computer without a watermark so if you go to the official Hunyan video avatar page and you scroll to the middle somewhere it does say that the minimum VRAM requirement is 24 GB but it's going to be very slow and they recommend using a GPU with 96 GB of VRAM i mean who the hell has 96 GB but the nice thing is you don't actually need 24 GB so you can see here their latest update is it now supports a GPU with only 10 GB of VRAM with TC included this is a tool that speeds up your generation even faster thanks to W2GP so that's what we're going to install today so let's click on this link and this is by the goat deep beep meep props to this guy for building this really simple interface now even though it says one notice that they've actually merged Hunyan video avatar and also LTV into this interface by the way GP stands for GPU poor which is quite a brutal name like does it mean you're too poor to afford a better GPU i don't know anyways if you scroll down here there are two ways you can install Want 12GP one way is to install this app called Pinocchio and after that they provide a one-click installer for Want 12GP so if you want just a easy one-click way to install this go with this option but this might be a bit slower it might be more bloated and less customizable so I prefer to install this manually and that's what we're going to go over today now these are just some short instructions so let's scroll a bit down and look at the full installation guide so note that here this requires Python and don't worry I'm going to show you how to install these if you don't have it and it requires a compatible GPU which is at least this or newer so here are the instructions for these types of GPUs or if you have the 50 generation then here are the installation instructions so for me I'm going to go over how to install this using these instructions now the first step is we need to get clone this repository which requires you to have git installed on your computer if you don't here's how to install it if you already have git installed feel free to skip to the next section so all we got to do is download the latest release for whatever operating system you're using so I'm using Windows so I'm just going to click on download for Windows i'm running 64-bit so I'm going to click on this to download and it's now downloading this .exe file so once that's completed all we got to do is open that exe file and then follow the steps so I'm going to click on next i'm just going to go with the default install location which is program files/get so I'll click next for that and then I'm just going to leave this at the default and then I'm going to click next again and click next here we're just going to use the default settings for all of these there's a lot of settings that you need to go through so I'm just going to click next for all of these all right and then it should go ahead and install all the files so this might take a few minutes perfect so now we have git installed all right assuming you have git installed the next step is to choose where on my computer I want this installed so let's say I just want to clone this on my desktop well I just need to open desktop and then at the top here type in cmd to open up my desktop in command prompt as you can see here now the next step is to copy this line and paste it in here so this is basically going to clone all the files and folders that you see over here into a folder on your desktop so let me just drag this over here and if I open this notice that it has the same files and folders that you see in this GitHub repo all right the next step is we need to change the directory to this new folder because right now we are still in desktop we need to go one folder in into this one 12GP folder so let me copy this line and then paste it in here so right now we are within this new W2GP folder the next step is we need to use to create a virtual environment called W2GP and this is going to use Python 3.10.9 now this does require you to have installed on your computer if you don't here's how to install it if you already have this installed feel free to skip to the next section now I'm just on anaconda.com and actually what I'm going to do is install Minionda this is a minimalist version of Anaconda if you install the full Anaconda it installs a lot of packages and dependencies that you might not need this just takes up more room on your computer and of course the installation time is a bit longer but with Minionda it's just a barebones package and you can always install additional packages and dependencies afterwards so I'm going to click on latest Minionda installer links by Python version and I'm using Windows so I'm going to install one of these now for free and open source AI tools usually they do not support Python 3.12 so it's better to install the Python 3.11 version so I'm going to click on this which should download an .exe file to your computer once it's finished downloading simply doubleclick on this and then follow the steps to complete the installation so I'm going to click next and then agree and then let's set this to all users i'm going to go with the default destination folder and then I'm going to check this as well clear the package cache upon completion this just gives you back some more disk space without affecting functionality all right once that's completed let's click next and then we are finished now we aren't done yet so if you open up the command prompt and you type in- version you're still going to see that is not recognized this is because we haven't added Anaconda to our path yet so let's exit out of this and then to add it to our path we simply search for this function edit the system environment variables we're going to click on this and then click on environment variables and then click on the one that says path and then click edit and here's where you add in the path of Anaconda so it depends where you installed Anaconda for me I installed it in program data so it's going to be in program data/min and then if I doubleclick on scripts you can see that is here so this is the folder we want to paste in so I'm going to rightclick on this and then copy as path and then back in the environment variables window I'm going to click new and then paste in the path here and then click okay and then okay and then okay again now if you open up command prompt again and type in- version you should see that we are running 24.5.0 so this shows that we have successfully installed Anaconda all right assuming you do have installed on your computer let's copy this line and then paste it in here so again this is just creating a new virtual environment called W2GP which uses Python 3.10 now the point of creating a virtual environment is think of it as like a separate hard drive that houses all the packages and dependencies that are required for this AI tool to work and this is important because you don't want any of this to conflict with existing packages or dependencies that you have on your computer all right so let's press Y to proceed all right afterwards the next step is we need to use to activate the virtual environment which we named 12GP so let's copy this and then paste it in here so now you can see that we are within our virtual environment because we have the name within parenthesis at the start of the line all right the next step is we need to install PyTorch now this does require you to have CUDA 12.4 or a later version on your system because it is backwards compatible so to check what version of CUDA you have simply open up another command prompt window and then type in NVCC- version and notice that it says here I have CUDA 12.4 so perfect now if you have an older version than CUDA 12.4 then this line might not work and you'll need to go to the PyTorch page and select the appropriate CUDA that you have so for example let's say you're using Windows and you have CUDA 11.8 then you'll need to use this CU118 instead of CU24 anyways I do have 12.4 so I'm going to copy this line and then paste it in here all right so this is going to install Torch Torch Vision and Torch Audio which takes like over 2 GB all right now after installing everything you should see this line again with no error message so the next step is to pip install all the requirements that are listed in this requirements.ext file so let me just open this really quickly and notice that it's a really long list of dependencies that it needs to run so let's copy this line and then paste it in here now again because this is such a long list it's going to take a while to download everything all right so it's going to proceed to install a really long list of packages but if all goes well you should see this line with no error messages now we can already end it there and start running this but here are some additional performance optimizations if we install Sage 2 attention we can generate videos 40% faster so let's go ahead and do this first we need to pip install Triton for Windows so let me copy this line and paste it in here all right so it says successfully installed Triton perfect the next step is we need to install this Sage Attention Wheel now even though it says CUDA 12.6 notice that CUDA is backwards compatible so it still works even if I have CUDA 12.4 anyways let me copy this and then paste it in here perfect so it has successfully installed Sage Attention and that's pretty much it we are now done with installing this so let me exit out of this window and show you how you can start this from scratch the next day so let's double click into our one 2GP folder and then at the top here let's type in cmd to open this folder up in command prompt and then afterwards we need to use cond to activate the virtual environment which we created which is called one togp and after that's done if you look at this GitHub repo if you only want to do text to video you can run this line but for us for Hunan avatar it works best if we upload an image as a reference so we're going to use image to video in which case we need to run this line so let me copy this and paste it in here now for the first time you run this you're going to have to wait a few more extra minutes for it to download this thing but afterwards you should see this URL so simply hold control and click on this link and it would open up this 1GP interface in your browser now even though this is using your internet browser it's not actually online so this is completely offline this is a local URL there's actually a ton of options here like one image to video and also phantom first frame last frame there's also sky reels and LTX video all of which I've gone over on my channel before plus we have this Hunyan video avatar down here so let me click on this now there are a few settings you should configure before you start running this so let's go over configuration and here there's a ton of different settings which you can play around with but we're going to click on this performance tab and you can change this up depending on how much VRAMm you have for example over here this VAE tiling if you enable this it will be slower but it will reduce the VRAMm requirements for me because I do have enough VRAM I'm going to disable this and then we also have this boost option which gives you around a 10% speed up without losing quality so yes let's go with this and then here for the profile you can select a ton of these profiles so the default is this one if you have low RAM and low VRAM and here it lists out the minimum amount that you should have so if you have at least 32 GB of RAM and 12 GB of VRAM you should select this if you have 24 GB of VRAM for example like if you have a 4090 then you should select profile 3 conversely if you have a ton of RAM but low VRAM for example if you have at least 48 GB of RAM and only 12 GB of VRAM then you would select this and so on and so forth these are pretty self-explanatory and then after you've selected the best options for your device you can click on apply changes all right so back here in video generator again we're going to use Hunen video avatar and here's where we upload a reference image and here is where we upload the audio for it to sync to and here's where we will enter a prompt to describe the scene further here is the aspect ratio so you can choose from a ton of different aspect ratios the nice thing about Hunyan video avatar is it supports up to even 1080p resolution it seems like and then over here is basically the length of your video so it's at roughly 25 frames per second so if you want to set it to 5 seconds then it would be like 125 and this can go all the way up to 41 frames which would be roughly 16 seconds which is more than enough and then over here is the number of inference steps this is basically how many steps the AI model should go through when generating your video in general the more steps you have the higher quality and more consistent your video will be it will also contain fewer errors however at a certain point like if you drag this to 100 steps for example that's going to be way too much you're going to get diminishing returns somewhere around like 30 to 40 steps and if you set this lower it's going to run faster but at the sacrifice of some quality all right and then over here we also have this advanced mode toggle and if we look at this here is where you can select the seed or basically the starting point of your generation here's the number of videos per prompt here is the guidance scale so how literally it should follow your prompt and then you can also add luras over here if you want if you're not familiar with the term Laura these are basically like fine-tuned models or special effects that you can add on top of your video generator if you want to generate for example a particular effect or motion or character now the most important tab in this advanced mode setting is this speed tab so here is where you can turn on or off tcash this makes your generation even faster so for example you can turn this on to speed up the video generation by up to 2.5x but at the sacrifice of some quality so I think around like a 2x speed up would be the right balance between speed and quality so let's select that and notice that this does also consume VRAM so make sure you do have enough VRAM before you turn this on and we also have like some upsampling settings here and quality and miscellaneous feel free to experiment and play around with these and that's pretty much it so let's start generating a video right now so for example I'm going to upload this image and for the audio let's use this simple one did you hear that i'm scared it sounded like something moving in the dark for the prompt I'm just going to write she is talking let's click generate and see what that gives us now when you run this for the first time notice that it also needs to download this Hungyian video avatar model which you can see down here so this is going to be quite a large file again it's like several gigabytes so it's going to take a while and here's what we get so notice that for this local tool it does not contain a watermark did you hear that i'm scared it sounded like something moving in the dark not the fastest video generator out there but this is because it includes audio and it needs to also lip-sync the video so there are a few extra steps in this process in a nutshell that is how you can install and run Hunyan Video Avatar locally on your computer even if you have low VRAM so that sums up my review and installation tutorial of Hunyan Video Avatar this is the first available open-source tool that can animate any scene including a person's full body with an audio clip it can even handle emotions and different expressions and different animation styles so it's a super powerful tool and best of all you can run it on as low as 10 GB of VRAM so definitely try this out i'll link to everything in the description below let me know what you think of this and if you encounter any errors during the installation welcome to paste your error message in the comments below and I'll try to help you troubleshoot as much as possible as always I will be on the lookout for the top AI news and tools to share with you so if you enjoyed this video remember to like share subscribe and stay tuned for more content also there's just so much happening in the world of AI every week I can't possibly cover everything on my YouTube channel so to really stay up tod date with all that's going on in AI be sure to subscribe to my free weekly newsletter the link to that will be in the description below thanks for watching and I'll see you in the next one